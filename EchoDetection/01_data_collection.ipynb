{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23cf0efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import unicodedata\n",
    "\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import spacy\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "reddit = praw.Reddit(\"learning\")\n",
    "header = {\"User-Agent\": \"my-bot/0.0.1 by u/Karmz0a\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a719c7",
   "metadata": {},
   "source": [
    "add demo mode that only uses 15 posts\n",
    "Input String + Search Subreddits\n",
    "filter out meme subs, make sure theyre relevant, check for satire\n",
    "search directly for comments, not posts -> comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "326ab0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = input()\n",
    "demo = False\n",
    "\n",
    "subreddit = reddit.subreddit(topic)\n",
    "count = 10 if demo else 10 if subreddit.subscribers > 10000000 else 100\n",
    "non_sticky = []\n",
    "i = 0\n",
    "for submission in subreddit.hot(limit = 125):\n",
    "    if submission.stickied or submission.num_comments == 0:\n",
    "        continue\n",
    "    else:\n",
    "        i = i + 1\n",
    "        non_sticky.append(submission)\n",
    "        if i == count:\n",
    "            break\n",
    "\n",
    "data = []\n",
    "for submission in non_sticky:\n",
    "    \n",
    "    submission.comments.replace_more(limit = None)\n",
    "    commentList = submission.comments.list()\n",
    "    for comment in commentList:\n",
    "        data.append({\n",
    "            \"author\": comment.author,\n",
    "            \"original post\" : submission.id,\n",
    "            \"upvotes\": comment.score,\n",
    "            \"body\": emoji.demojize(comment.body, delimiters=(\":\", \":\")).replace(\"\\n\", \" \").replace(\"\\r\", \" \"),\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26b622c",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "151c3be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pweir\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: divide by zero encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "C:\\Users\\pweir\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\arraylike.py:399: RuntimeWarning: invalid value encountered in log10\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "commentFrame = pd.DataFrame(data)\n",
    "\n",
    "commentFrame = commentFrame[(commentFrame[\"author\"] != \"AutoModerator\") & \n",
    "                            (commentFrame[\"author\"].notna()) &\n",
    "                            (commentFrame[\"body\"] != \"[removed]\")\n",
    "                            ].drop_duplicates()\n",
    "\n",
    "commentFrame[\"body\"] = commentFrame[\"body\"].apply(lambda x: unicodedata.normalize(\"NFC\", x ))\n",
    "\n",
    "commentFrame[\"upvoteScale\"] = np.where(\n",
    "    commentFrame[\"upvotes\"] == 0,\n",
    "    0,\n",
    "    np.log10(commentFrame[\"upvotes\"] + 1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45d4924",
   "metadata": {},
   "source": [
    "Sentiment Analysis\n",
    "give user a list of spaCy labels to choose what is relevant and what is not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf4e0c",
   "metadata": {},
   "source": [
    "filter SPACY keywords -> work of art, product, org person etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "011a63c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d87c05d50e3743f0a667b28119660b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable= [\"parser\", \"tagger\"])\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "spacy_entities = [\"ORG\", \"PRODUCT\", \"PERSON\", \"WORK_OF_ART\"] # make this multiple choice for the user\n",
    "entity = []\n",
    "for doc in nlp.pipe(commentFrame[\"body\"], batch_size= 100, n_process=-1):\n",
    "    for ent in doc.ents:\n",
    "        entity.append((ent.text, ent.label_))\n",
    "\n",
    "\n",
    "df_entities = pd.DataFrame(entity, columns=[\"entity\", \"label\"])\n",
    "e = df_entities[df_entities[\"label\"].isin(spacy_entities)].value_counts().head(18)\n",
    "top_ent = [idx[0] for idx in e.index]\n",
    "\n",
    "\n",
    "comments = commentFrame[\"body\"].tolist()\n",
    "query_emb =  embedder.encode(top_ent, normalize_embeddings= True)\n",
    "\n",
    "comment_emb = embedder.encode(comments, normalize_embeddings= True, show_progress_bar= True, batch_size= 16)\n",
    "\n",
    "relevance = embedder.similarity(query_emb, comment_emb)\n",
    "best_score = torch.max(relevance, dim = 0).values.tolist()\n",
    "\n",
    "relevance = embedder.similarity(query_emb, comment_emb)\n",
    "best_score = torch.max(relevance, dim = 0).values.tolist()\n",
    "commentFrame[\"relevance\"] =  best_score\n",
    "\n",
    "cutoff = np.percentile(commentFrame[\"relevance\"], 10)\n",
    "commentFrame = commentFrame[commentFrame[\"relevance\"] > cutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef8073a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "sentiment analysis: 100%|██████████| 1218/1218 [16:26<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                author original post  upvotes  \\\n",
      "0         linkling1039       1nk7ow2        1   \n",
      "1              gomster       1nk7ow2        1   \n",
      "2          Jafranci715       1nk7ow2        1   \n",
      "3    Salt-Analysis1319       1nk7ow2        1   \n",
      "4  A-Centrifugal-Force       1nk7ow2        1   \n",
      "\n",
      "                                                body  upvoteScale  relevance  \\\n",
      "0  I'm really jealous of the people that got to p...      0.30103   0.159744   \n",
      "1                It’s crazy how well Galaxy holds up      0.30103   0.175538   \n",
      "2       Does the switch 2 version launch on off 2nd?      0.30103   0.620313   \n",
      "3  As someone who hasn't played either game since...      0.30103   0.547247   \n",
      "4  They look so good! Can’t wait to play both of ...      0.30103   0.188442   \n",
      "\n",
      "  sentimentLabel  sentimentScore  \n",
      "0       POSITIVE        0.989360  \n",
      "1       NEGATIVE        0.974828  \n",
      "2        NEUTRAL        0.518872  \n",
      "3        NEUTRAL        0.573924  \n",
      "4       POSITIVE        0.993539  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "comments = commentFrame[\"body\"].tolist()\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "classifier = pipeline(\"sentiment-analysis\", model = \"AG6019/reddit-comment-sentiment-final\", device = device)\n",
    "\n",
    "label_map = {\"LABEL_0\": \"NEGATIVE\", \"LABEL_1\": \"POSITIVE\"}\n",
    "\n",
    "batch_size = 8\n",
    "results = []\n",
    "\n",
    "for i in tqdm(range(0, len(comments), batch_size), desc = \"sentiment analysis\"):\n",
    "    batch = comments[i:i+batch_size]\n",
    "    sentiment = classifier(batch, truncation = True, max_length = 128)\n",
    "    results.extend(sentiment)\n",
    "\n",
    "commentFrame[\"sentimentLabel\"] = [\"NEUTRAL\" if r[\"score\"] < 0.85 else label_map[r[\"label\"]] for r in results]\n",
    "commentFrame[\"sentimentScore\"] = [r[\"score\"] for r in results]\n",
    "\n",
    "print(commentFrame.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d2458f",
   "metadata": {},
   "source": [
    "Account for Sarcasm and Upvotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68993c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentimentLabel\n",
      "POSITIVE    5320\n",
      "NEUTRAL     2783\n",
      "NEGATIVE    1639\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(commentFrame[\"sentimentLabel\"].value_counts())\n",
    "commentFrame.to_pickle(\"comments.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca544a3",
   "metadata": {},
   "source": [
    "sentiment assortivity\n",
    "overall sentiment of the subreddit- battle ground or agreement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
